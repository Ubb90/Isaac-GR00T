apiVersion: batch/v1
kind: Job
metadata:
  generateName: groot-finetune-
  namespace: pgr25florent
spec:
  backoffLimit: 0 # Don't retry automatically if code fails
  template:
    spec:
      serviceAccountName: containerroot
 
      # Hardware Request
      nodeSelector:
        nvidia.com/gpu.product: "NVIDIA-RTX-A6000"
 
      volumes:
      # 1. Output Storage (Personal)
      - name: personal-storage
        persistentVolumeClaim:
          claimName: pgr25florentvol1claim
      
      - name: ssh-key
        secret:
          secretName: launcher-ssh-key
          defaultMode: 256

      # 2. Input Data Storage (Shared)
      - name: shared-storage
        persistentVolumeClaim:
          claimName: pgr25florent-datasets-claim
 
      - name: dshm
        emptyDir:
          medium: Memory
 
      containers:
      - name: trainer
        image: docker.io/fkgsoftware/groot-finetune:v1
        imagePullPolicy: Always
 
        securityContext:
          runAsUser: 0
 
        resources:
          requests:
            cpu: "16"
            memory: "64Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "24"
            memory: "128Gi"
            nvidia.com/gpu: "1"
 
        volumeMounts:
        - name: personal-storage
          mountPath: /data
        - name: shared-storage
          mountPath: /mnt/datasets
          readOnly: true
        - name: dshm
          mountPath: /dev/shm
        - name: ssh-key
          mountPath: /root/.ssh

        env:
        - name: WANDB_MODE
          value: "offline"
        - name: PYTHONUNBUFFERED
          value: "1"
 
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "--- STARTING GR00T FINETUNE ---"
            nvidia-smi
            git remote remove origin
            git remote add origin https://github.com/Ubb90/Isaac-GR00T.git
            git stash
            git pull origin main
            
            # 1. Verify Dataset Path before starting
            DATASET_PATH="/mnt/datasets/letrack/so101track_cube_swap_lerobot_random/"
 
            if [ ! -d "$DATASET_PATH" ]; then
              echo "ERROR: Dataset not found at $DATASET_PATH"
              echo "Listing /mnt/datasets/letrack to help debug:"
              ls -F /mnt/datasets/letrack/
              exit 1
            fi
 
            echo "Dataset found at: $DATASET_PATH"
 
            # 2. Create Output Directory on Personal Storage
            OUTPUT_DIR="/data/groot_track_cube_offset_exp1"
            mkdir -p "$OUTPUT_DIR"
 
            # 3. Run Training
            python scripts/gr00t_finetune.py \
              --dataset-path "$DATASET_PATH" \
              --output-dir "$OUTPUT_DIR" \
              --num-gpus 1 \
              --max-steps 50000 \
              --batch-size 32 \
              --learning_rate 1e-5 \
              --video-backend torchvision_av \
              --data-config so100_track \
              --no-tune-visual \
              --lora-rank 2 \
              --lora-alpha 128 \
              --lora-dropout 0.05 \
              --enable_early_stopping \
              --early_stopping_patience 100 \
              --early_stopping_threshold 0.01 \
              --eval_steps 50 \
              --weight_decay 1e-6
              2>&1 | tee "$OUTPUT_DIR/training_log.txt"
 
      restartPolicy: OnFailure